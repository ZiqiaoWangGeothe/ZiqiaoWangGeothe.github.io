# jemdoc: menu{MENU}{papers.html}, nofooter
== Publications \& Preprints

\* denotes equal contribution.
- *Ziqiao Wang* and Yongyi Mao, ``Tighter Information-Theoretic Generalization Bounds from Supersamples'', Submitted \[[https://arxiv.org/abs/2302.02432 Arxiv]\].
- Zixuan Liu\*, *Ziqiao Wang*\*, Hongyu Guo, and Yongyi Mao, ``Over-Training with Mixup May Hurt Generalization'', International Conference on Learning Representations (ICLR) 2023 \[[https://openreview.net/pdf?id=JmkjrlVE-DG Paper]\].
- *Ziqiao Wang* and Yongyi Mao, ``Information-Theoretic Analysis of Unsupervised Domain Adaptation'', International Conference on Learning Representations (ICLR) 2023 \[[https://openreview.net/pdf?id=c5tbxWXU9-y Paper]\]\[[https://arxiv.org/abs/2210.00706 Arxiv]\].
- *Ziqiao Wang* and Yongyi Mao, ``Two Facets of SDE Under an Information-Theoretic Lens: Generalization of SGD via Training Trajectories and via Terminal States'', Submitted \[[https://arxiv.org/abs/2211.10691 Arxiv]\].
- Zixuan Liu\*, *Ziqiao Wang*\*, Hongyu Guo, and Yongyi Mao, ``Over-Training with Mixup May Hurt Generalization'', First Workshop on Interpolation Regularizers and Beyond at NeurIPS 2022 \[[https://openreview.net/pdf?id=dh462LeVbh Paper]\]\[[posters/NeurIPS2022MixupPoster.pdf Poster]\].
- *Ziqiao Wang* and Yongyi Mao, ``On the Generalization of Models Trained with SGD: Information-Theoretic Bounds and Implications'', International Conference on Learning Representations (ICLR) 2022 \[[https://openreview.net/pdf?id=oWZsQ8o5EA Paper]\]\[[https://arxiv.org/abs/2110.03128 Arxiv]\]\[[posters/ICLR2022Poster.pdf Poster]\].
- *Ziqiao Wang*, Yongyi Mao, Hongyu Guo, and Richong Zhang, ``On SkipGram Word Embedding Models with Negative Sampling: Unified Framework and Impact of Noise Distributions'', arXiv preprint arXiv:2009.04413, 2020 \[[https://arxiv.org/abs/2009.04413 Arxiv]\]\[[posters/Ottawa_AI.pdf Poster]\].

