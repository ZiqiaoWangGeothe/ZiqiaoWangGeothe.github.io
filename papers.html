<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>Publications & Preprints</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Home</div>
<div class="menu-item"><a href="index.html">About&nbsp;me</a></div>
<div class="menu-item"><a href="news.html">News</a></div>
<div class="menu-item"><a href="papers.html" class="current">Papers</a></div>
<div class="menu-item"><a href="teaching.html">Teaching</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>Publications & Preprints</h1>
</div>
<p>* denotes equal contribution.</p>
<ul>
<li><p><b>Ziqiao Wang</b> and Yongyi Mao, &lsquo;&lsquo;Tighter Information-Theoretic Generalization Bounds from Supersamples&rsquo;&rsquo;, Submitted [<a href="https://arxiv.org/abs/2302.02432">Arxiv</a>].</p>
</li>
<li><p>Zixuan Liu*, <b>Ziqiao Wang</b>*, Hongyu Guo, and Yongyi Mao, &lsquo;&lsquo;Over-Training with Mixup May Hurt Generalization&rsquo;&rsquo;, International Conference on Learning Representations (ICLR) 2023 [<a href="https://openreview.net/pdf?id=JmkjrlVE-DG">Paper</a>].</p>
</li>
<li><p><b>Ziqiao Wang</b> and Yongyi Mao, &lsquo;&lsquo;Information-Theoretic Analysis of Unsupervised Domain Adaptation&rsquo;&rsquo;, International Conference on Learning Representations (ICLR) 2023 [<a href="https://openreview.net/pdf?id=c5tbxWXU9-y">Paper</a>][<a href="https://arxiv.org/abs/2210.00706">Arxiv</a>].</p>
</li>
<li><p><b>Ziqiao Wang</b> and Yongyi Mao, &lsquo;&lsquo;Two Facets of SDE Under an Information-Theoretic Lens: Generalization of SGD via Training Trajectories and via Terminal States&rsquo;&rsquo;, Submitted [<a href="https://arxiv.org/abs/2211.10691">Arxiv</a>].</p>
</li>
<li><p>Zixuan Liu*, <b>Ziqiao Wang</b>*, Hongyu Guo, and Yongyi Mao, &lsquo;&lsquo;Over-Training with Mixup May Hurt Generalization&rsquo;&rsquo;, First Workshop on Interpolation Regularizers and Beyond at NeurIPS 2022 [<a href="https://openreview.net/pdf?id=dh462LeVbh">Paper</a>].</p>
</li>
<li><p><b>Ziqiao Wang</b> and Yongyi Mao, &lsquo;&lsquo;On the Generalization of Models Trained with SGD: Information-Theoretic Bounds and Implications&rsquo;&rsquo;, International Conference on Learning Representations (ICLR) 2022 [<a href="https://openreview.net/pdf?id=oWZsQ8o5EA">Paper</a>][<a href="https://arxiv.org/abs/2110.03128">Arxiv</a>].</p>
</li>
<li><p><b>Ziqiao Wang</b>, Yongyi Mao, Hongyu Guo, and Richong Zhang, &lsquo;&lsquo;On SkipGram Word Embedding Models with Negative Sampling: Unified Framework and Impact of Noise Distributions&rsquo;&rsquo;, arXiv preprint arXiv:2009.04413, 2020 [<a href="https://arxiv.org/abs/2009.04413">Arxiv</a>].</p>
</li>
</ul>
</td>
</tr>
</table>
</body>
</html>
